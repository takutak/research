# research
zattaにいろいろ書いてきたけど、そろそろ研究で必要な道具がそろってきたので作成。

# 内容

## PINNs
ニューラルネットによって、微分方程式の解を近似する。物理の支配方程式を損失に組み込むことで実現。今回は逆問題を解く。

## Meta Learning
- MAML?
- LEAP?
- hyper Network?
## Meta-Learning + PINNs
PINNsの難点として、非常に収束まで時間がかかること。計算コストが高い。
そのため、似た課題であればメタ学習で解消

## メタ学習
MAMLで求めたい勾配について解説。そのために、まずは基本的な設定を導入。
#### 問題設定
通常の深層学習では、あるタスクが与えられ、そのタスクにおける教師データが与えられることで、入出力関係をニューラルネットワークにより近似する。
一方で、メタ学習では与えられたタスクの教師データが少ない状況下で、より良い近似が可能なニューラルネットワークを学習する。  
つまり、メタ学習では、解きたいタスクにおいてはデータが不足しているが、似たタスクについては自由にデータをサンプルできる状況を考える。このために、タスクの分布$p(\tau)$を考える。  
MAMLにおいては、タスク$\tau_i \sim p(\tau)$をサンプルして、このタスクを用いてニューラルネットワークの「良い」初期値を更新して求める。このために、一度タスク$\tau_i$について、通常の深層学習のような学習を行い(inner-loop)、その後、inner-loopで求めたパラメタを用いて、初期値の更新(outer-loop)を行う。  
最終的に、解きたいタスクは、「良い」初期値から行われ、それによって少データでも高速に収束する。
#### 初期パラメータの「良さ」とは
MAMLにおける「良い」初期値とは、最適化を行なった結果、損失が最小になるようなパラメータである。（このパラメータは、必ずしも最適なパラメータと近い距離にあるとは限らない。）数式として表現する。
ニューラルネットワークのパラメータを$\phi$とし、AdamやSGDといった最適化手法の一回のイテレーションを表す演算子を$U_{\tau}$と表記する。すると、求めるパラメータは、
$$ minimize_{\phi} \mathbb{E}_\tau [L_\tau(U_\tau(\phi))]$$
MAMLにおける勾配$g_{maml}$は、
$$
\begin{aligned}
g_{maml} &= \frac{d}{d\phi} L_\tau(U_\tau(\phi)) \\
&= U_\tau'(\phi) \times L_\tau'(\tilde{\phi})
\end{aligned}
$$


#### innner-loopでk回のパラメータ更新(SGD)を行う場合(テイラー展開)
inner-loopでk回のパラメータ更新を行うとする。諸々を次のように表記。
- それぞれの損失関数を$L_i, i \in \{1,2..k\}$
- $i-1$回目のパラメータ更新を行なった後のパラメータを$\phi_i$で表記。  
 また、この時$i$回目の更新時における勾配は、
$$ g_i = L_i'(\phi_i) $$
- SGDでパラメータ更新をするので、パラメータの更新式は、
  $$ \phi_{i+1} = \phi_i - \alpha g_{i} = U_i(\phi_i)$$
- 初期パラメータ$\phi_1$における、i番目の損失関数の勾配
$$\bar{g_i}  = \frac{d}{d\phi_1}L_i(\phi_1)$$
- 初期パラメータ$\phi_1$における、i番目の損失関数のヘッセ行列
$$ \bar{H_i} = L_i''(\phi_1)$$

この元で、MAMLの勾配を計算する。
$$
\begin{aligned}
g_{MAML} &= \frac{d}{d\phi_1}L_k(\phi_k) \\
&= L_k'(\phi_k) \frac{d\phi_k}{d\phi_1}
\end{aligned}
$$
ここで、
$$ \begin{aligned}
\phi_k &= U_{k-1}(\phi_{k-1}) \\
&=  U_{k-1}(U_{k-2}(\phi_{k-2}) ) = ... \\
&= U_{k-1}(U_{k-2}( ... U_1 (\phi_{1})) ) 
\end{aligned}
$$
よって、
$$
\begin{aligned}
\frac{d\phi_k}{d\phi_1} &= 

\end{aligned}